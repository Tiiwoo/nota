%import{./diagrams}{SyntaxDiagram}{AssignStaticRule}{AssignDynamicRule}
%import{./language}{L}{L2}{msf}

%letfn{tc}{#tex_ref{tc}{#0; #1; #2 \vdash #3 : #4 \Rightarrow #5}}
%letfn{subtype}{#tex_ref{subtype}{#0; #1 \vdash #2 \mathrel{\footnotesize \lesssim} #3 \Rightarrow #4}}
%letfn{ownsafe}{#tex_ref{ownsafe}{#0; #1 \vdash_{#2} {#3} \Rightarrow {#4}}}

%let{r}{#L.concrprov{}}
%let{uniq}{#L.ownquniq{}}
%let{shrd}{#L.ownqshrd{}}
%let{uty}{#L.tybnum{}}
%let{loanset}{\{\overline{#L2.loan{}}\}}

@Title{Modular Program Slicing Through Ownership}

@Section[label="sec:intro"]{Introduction}

Program slicing is the task of identifying the subset of a program relevant to computing a value of interest. The concept of slicing was introduced 40 years ago when @Ref[full]{weiser1982programmers} demonstrated that programmers mentally construct slices while debugging. Since then, hundreds of papers have been published on implementing automated program slice, as surveyed by @Ref[full]{xu2005brief} and @Ref[full]{silva2012vocabulary}. Despite these efforts, a review of slicers found "slicing-based debugging techniques are rarely used in practice" @Ref{parnin2011automated}@Ref{footnote:1}.

@FootnoteDef{
  The only open-source, functioning slicers the authors could find are Frama-C 
  @Ref{cuoq2012frama} and dg @Ref{llvmslicer}. Slicing tools for Java like Kaveri 
  @Ref{jayaraman2005kaveri} no longer work. The most industrial-strength slicing tool, 
  CodeSurfer @Ref{balakrishnan2005codesurfer} was GrammaTech's proprietary technology 
  and appears to no longer exist.
}

The standard solution for analyzing programs with pointers and functions is @em{whole-program analysis}. That is, for a given function of interest, analyze the definitions of all of the function's callers and callees in the current codebase. However, whole-program analysis suffers from a few logistical and conceptual issues:

@ul{
  @li{Analysis time scales with the size of the whole program:_ the time complexity of whole-program analysis scales either polynomially or exponentially with the number of call sites in the program, depending on context-sensitivity @Ref{might2010resolving}. In practice, this means more complex codebases can take substantially longer to analyze. For instance, the recent PSEGPT pointer analysis tool @Ref{zhao2018parallel} takes 1 second on a codebase of 282,000 lines of code and 3 minutes on a codebase of 2.2 million lines of code.}
  @li{@em{Analysis requires access to source code for the whole program:} an assumption of analyzing a whole program is that a whole program is actually accessible. However, many programs use libraries that are shipped as pre-compiled objects with no source code, either for reasons of efficiency or intellectual property.}
  @li{@em{Analysis results are anti-modular:} when analyzing a particular function, relying on calling contexts to analyze the function's inputs means that any results are not universal. Calling-context-sensitive analysis determine whether two pointers alias @em{in the context of the broader codebase}, so alias analysis results can change due to modifications in code far away from the current module.}
}

These issues are not new --- @Ref[full]{rountev1999data} and @Ref[full]{cousot2002modular} observed the same two decades ago when arguing for modular static analysis. The key insight arising from their research is that static analysis can be modularized by computing @em{symbolic procedure summaries}. For instance, @Ref[full]{yorsh2008generating} show how to automatically summarize which inputs and outputs are possibly null for a given Java function. The analysis is modular because a function's summary can be computed only given the summaries, and not definitions, of callees in the function. In such prior work, the language of symbolic procedure summaries has been defined in a separate formal system from the programming language being analyzed, such as the micro-transformer framework of @Ref[full]{yorsh2008generating}.

Our work begins with the observation: @em{function type signatures are symbolic procedure summaries}. The more expressive a language's type system, the more behavior that can be summarized by a type. Nearly all work on program slicing, dataflow analysis, and procedure summaries has operated on C, Java, or equivalents. These languages have impoverished type systems, and so any interesting static analysis requires a standalone abstract interpreter. However, if a language's type system were expressive enough to encode information about dataflow, then a function's type signature could be used to reason about the aliasing and side effects needed for slicing. Moreover, a function's type signature is required information for a compiler to export when building a library. Using the type system for dataflow analysis therefore obviates the logistical challenge of integrating an external analysis tool into a complex build system.

Today, the primary technique for managing dataflow with types is @em{ownership}. Ownership is a concept that has emerged from several intersecting lines of research on linear logic @Ref{girard1987linear}, class-based alias management @Ref{clarke1998ownership}, and region-based memory management @Ref{grossman2002region}. Generally, ownership refers to a system where values are owned by an entity, which can temporarily or permanently transfer ownership to other entities. The type system then statically tracks the flow of ownership between entities. Ownership-based type systems enforce the invariant that values are not simultaneously aliased and mutated, either for the purposes of avoiding memory errors, data races, or abstraction violations

Our thesis is that ownership can modularize program slicing by using types to compute a provably sound and reasonably precise approximation of the necessary dataflow information. We build this thesis in five parts:

@ol{
  @li{We provide an intuition for the relationship between ownership and slicing by describing how ownership works in Rust, the only industrial-grade ownership-based programming language today (@Ref{sec:background}).}
  @li{We formalize an algorithm for modular static slicing as an extension to the type system of Oxide @Ref{weiss2019oxide}, a formal model of Rust's static and dynamic semantics (@Ref{sec:model} and @Ref{sec:algorithm}).}
  @li{We prove the soundness of this algorithm as a form of noninterference, building on the connection between slicing and information flow established by @Ref[full]{abadi1999core} (@Ref{sec:soundness} and @Ref{sec:appendix})}
  @li{We describe an implementation of the slicing algorithm for Rust, translating the core insights of the algorithm to work on a lower-level control-flow graph (@Ref{sec:implementation})}
  @li{We evaluate the precision of the modular Rust slicer against a whole-program slicer on a dataset of 10 codebases with a total of 280k LOC. We find that modular slices are the same size as whole-program slices 95.4\% of the time, and are on average 7.6\% larger in the remaining 4.6\% of cases (@Ref{sec:evaluation}).}
}

@Section[label="sec:background"]{Principles}

A backwards static slice is the subset of a program that could influence a particular value (backwards) under any possible execution (static). A slice is defined with respect to a slicing criterion, which is a variable at a particular point in a program. In this section, we provide an intuition for how slices interact with different features of the Rust programming language, namely: places (@Ref{sec:places}), references (@Ref{sec:pointers}), function calls (@Ref{sec:funcalls}), and interior mutability (@Ref{sec:intmut}).

@Subsection{Places}

A place is a reference to a concrete piece of data in memory, like a variable x or path into a data structure x.field. Slices on places are defined by bindings, mutation, and control flow.

@Wrap{@Listing{
  let mut x = 1;
  let y = 2;
  let z = 3;
  x = y;
  println!("{}", $x$);
}}

For instance, the Rust snippet on the right shows the slice in orange of a place in green. The assignment @code{x = y} means @code{y} is relevant for the slice, so the statement @code{let y = 2} is relevant as well. Because @code{z} is not used in the computation of @code{x}, then @code{let z = 3}. is not relevant. Additionally, because @code{x = y} overwrites the previous value of @code{x}, then the original assignment @code{x = 1} is not relevant either.

[...]

@Row{
  @Listing{
    let mut t = (0, 1, 2);
    t = (3, 4, 5);
    t.0 = 6;
    t.1 = 7;
    println!("{:?}", $t$);
  }
  @Listing{
    let mut t = (0, 1, 2);
    t = (3, 4, 5);
    t.0 = 6;
    t.1 = 7;
    println!("{}", $t.0$);
  }
  @Listing{
    let mut t = (0, 1, 2);
    t = (3, 4, 5);
    t.0 = 6;
    t.1 = 7;
    println!("{}", $t.2$);
  }
}

@Section{Formal Model}

To build an algorithm from these principles, we first need a formal model to describe and
reason about the underlying language. Rather than devise our own, we build on the work of
@Ref[full]{weiss2019oxide} : Oxide is a model of (safe) Rust's surface language with a
formal static and dynamic semantics, along with a proof of syntactic type soundness.
Importantly, Oxide uses a provenance model of lifetimes which we leverage for our slicing
algorithm.

We will incrementally introduce the aspects of Oxide's syntax and semantics as necessary to understand our principles and algorithm. We describe Oxide's syntax (@Ref{sec:syn}), static semantics (@Ref{sec:statsem}) and dynamic semantics (@Ref{sec:dynsem}), and then apply these concepts to formalize the slicing principles of the previous section
(@Ref{sec:formal_principles}).

@Subsection{Syntax}

@Ref{fig:oxide_syntax} shows a subset of Oxide's syntax along with a labeled example. An Oxide program consists of a set of functions @${#L.fenv{}} (the "global environment"), where each function body is an expression @${#L.expr{}}.

@Figure[label="fig:oxide_syntax"]{
  @Subfigure{
    @L.Bnf
    @L2.Bnf
    @Caption{Subset of Oxide syntax, reproduced from @Ref[full]{weiss2019oxide}. The only difference in this subset is that closures are eliminated and functions are simplified to take one argument.}
  }
  @Subfigure[label="fig:oxide_syntax_example"]{
    @SyntaxDiagram
    @Caption{Syntactic forms and corresponding metavariables labeled in context of an example}
  }
  @Caption{Formal elements of Oxide and their explanation (excerpts).}
}

The syntax is largely the same as Rust's with a few exceptions: this is a bit funky

@ul{
  @li{Lifetimes are called "provenances", and they are both explicit in expressions and types throughout the program, and initially bound via @${#L.exprprov{}} expressions or as function parameters.}
  @li{Rather than having immutable references @code{&'a τ} and mutable references @code{&'a mut τ}, Oxide calls them "shared" references @${#L.tysref{#shrd}{#L.prov{}}{#L.tys{}}} and "unique" references @${#L.tysref{#r}{#L.prov{}}{#L.tys{}}.}}
  @li{Provenances are divided into "concrete" (@${#L.concrprov{}}) and "abstract" (@${#L.abstrprov{}}). Concrete provenances are used by borrow expressions, and abstract provenances are function parameters used for inputs with reference type.}
}

@Subsection[label="sec:statem"]{Static semantics}

@Definition[name="tex:tc"]{
  Expressions are typechecked via the judgment @${#tc{#L.fenv{}}{#L2.tyenv{}}{#L2.stackenv{}}{#L.expr{}}{#L2.ty{}}{#L2.stackenv{}'}}, read as: "@${#L.expr{}} has type @${#L2.ty{}}" under contexts @${#L.fenv{}, #L2.tyenv{}, #L2.stackenv{}}."
} 
@${#L2.tyenv{}} contains function-level type and provenance variables. @${#L2.stackenv{}} maps variables to types and provenances to pointed-to place expressions with ownership qualifiers. For instance, when type checking @code{*b := a.1} in @Ref{fig:oxide_syntax_example}, the inputs would be @${#L2.tyenv{} = #L2.tyenvempty{}} (empty) and 
@${
  #L2.stackenv{} = \{a \mapsto #L.tystup{#uty, #uty},~ 
  b \mapsto #L.exprref{#r _2}{#uniq}{#uty}, ~
  #r _1 \mapsto \{#L2.loanform{#uniq}{a.0}\}, ~
  #r _2 \mapsto \{#L2.loanform{#uniq}{a.0}\}\}
}.

Typechecking relies on a number of auxiliary judgments, such as subtyping (@Definition[name="tex:subtype"]{
  @${#subtype{#L2.tyenv{}}{#L2.stackenv{}}{#L2.ty{}_1}{#L2.ty{}_2}{#L2.stackenv{}'}}
}) and ownership-safety (@Definition[name="tex:ownsafe"]{
  @${#ownsafe{#L2.tyenv{}}{#L2.stackenv{}}{#L.ownq{}}{#L.pexp{}}{#loanset}}, read as "@${#L.pexp{}} has @${#L.ownq{}\text{-loans } #loanset} in the contexts @${#L2.tyenv{}, #L2.stackenv{}}"
}).
As an example, consider @Smallcaps{T-Assign} for the assignment expression @${#L.exprplcasgn{#L.plc{}}{#L.expr{}}}:

%import_default{./slicing-paper.bib}{bibtex}
@References{#bibtex}